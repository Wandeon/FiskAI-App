# Answer Composition Layer Audit Report

**Date:** 2024-12-26
**Auditor:** Claude (Opus 4.5)
**Scope:** Semantic correctness of answer composition layer
**Verdict:** **FAIL**

---

## Executive Summary

The answer composition layer has **critical gaps** in semantic validation between `directAnswer` content and cited evidence. While the system has strong structural guarantees (citations required, source pointers linked), it lacks semantic guards that prevent natural-language answers from introducing meaning not explicitly supported by evidence.

---

## Architecture Overview

```
Query → interpretQuery → matchConcepts → selectRules → buildCitations → buildAnswer
                                              ↓
                              RegulatoryRule.explanationHr → directAnswer
                                              ↓
                              (Generated by COMPOSER LLM agent)
```

**Key files:**

- `src/lib/assistant/query-engine/answer-builder.ts` - Main answer composition
- `src/lib/regulatory-truth/agents/composer.ts` - Rule creation with LLM
- `src/lib/regulatory-truth/prompts/index.ts` - LLM prompts
- `src/lib/assistant/validation.ts` - Response validation

---

## Invariant Verification

### Invariant 1: Every sentence in directAnswer maps to cited evidence

**Status:** FAIL

**Finding:** `directAnswer` is populated from `primaryRule.explanationHr` (line 395-396 in `answer-builder.ts`):

```typescript
directAnswer:
  primaryRule.explanationHr || formatValue(primaryRule.value, primaryRule.valueType),
```

The `explanationHr` field is generated by the COMPOSER LLM agent with the prompt instruction (line 224 in `prompts/index.ts`):

> "Write a human-readable explanation"

**Critical gap:** There is NO validation that `explanationHr` semantically matches `exactQuote` from SourcePointer. The Reviewer agent checks `value_matches_source` but this only validates the numeric `value` field, NOT the natural-language explanation text.

**Hallucination path:**

1. EXTRACTOR extracts value "25%" with exactQuote "stopa PDV-a iznosi 25%"
2. COMPOSER receives exactQuote but generates explanationHr: "U Hrvatskoj se **uvijek** primjenjuje standardna stopa PDV-a od 25% na **sve** proizvode"
3. The word "uvijek" (always) and "sve" (all) were not in the source quote
4. directAnswer displays the expanded meaning without validation

### Invariant 2: No modal verbs unless present in source quote

**Status:** FAIL

**Finding:** No modal verb validation exists anywhere in the codebase.

The EXTRACTOR_PROMPT (line 38) states:

> "Always preserve exact text, never paraphrase"

But this constraint applies only to `exactQuote` extraction, NOT to COMPOSER's `explanation_hr` generation.

**Risky modal verbs in Croatian:**

- `mora` / `moram` (must)
- `treba` / `trebam` (should/need to)
- `uvijek` (always)
- `nikad` (never)
- `obavezno` (required/mandatory)

**Evidence:** These words appear in clarification suggestions (answer-builder.ts:140, 496) and personalization detection (query-interpreter.ts:300-301), proving they're recognized as significant - yet no filter prevents their introduction into explanationHr.

### Invariant 3: No inferred thresholds, dates, or obligations without direct evidence

**Status:** FAIL

**Finding:** COMPOSER_PROMPT (line 224-227) states:

> "CONSTRAINTS:
>
> - Never invent values not present in SourcePointers"

But this constraint is:

1. **Unenforced** - no code validates that values in `explanation_hr` match `extractedValue`
2. **Limited to values** - doesn't cover dates, obligations, or qualitative statements
3. **Prompt-only** - relies on LLM compliance, not deterministic validation

**Concrete risk:** COMPOSER receives a SourcePointer with:

```json
{
  "extractedValue": "39816.84",
  "exactQuote": "prag od 39.816,84 EUR za paušalno oporezivanje"
}
```

Nothing prevents COMPOSER from generating:

```
"Godišnji prag za paušalni obrt iznosi 39.816,84 EUR, a prelazak praga **do kraja godine** zahtijeva prijavu u sustav PDV-a **unutar 8 dana**."
```

The deadline "8 dana" and temporal phrase "do kraja godine" were not in the source.

### Invariant 4: Confidence badges align with evidence strength

**Status:** PARTIAL FAIL

**Finding:** Confidence is derived from `primaryRule.confidence` (line 398-402 in `answer-builder.ts`):

```typescript
confidence: {
  level:
    primaryRule.confidence >= 0.9 ? "HIGH" : primaryRule.confidence >= 0.7 ? "MEDIUM" : "LOW",
  score: primaryRule.confidence,
}
```

**Issues:**

1. Confidence is rule-level, not evidence-level
2. No differentiation between multi-source vs single-source rules
3. A rule with 3 supporting sources shows the same confidence as a rule with 1 source (assuming same score)

**Partial pass:** The underlying confidence from REVIEWER does factor in source validation checks (value_matches_source, sources_complete), but this doesn't surface in the final confidence badge.

---

## Code Paths Where Meaning Can Be Expanded/Softened

### Path 1: COMPOSER explanationHr Generation

**File:** `src/lib/regulatory-truth/agents/composer.ts:318`

```typescript
explanationHr: draftRule.explanation_hr,
```

LLM-generated text stored directly without semantic validation against source quotes.

### Path 2: directAnswer Population

**File:** `src/lib/assistant/query-engine/answer-builder.ts:395-396`

```typescript
directAnswer:
  primaryRule.explanationHr || formatValue(primaryRule.value, primaryRule.valueType),
```

Uses explanationHr without comparing to exactQuote.

### Path 3: COMPOSER Prompt Lacks Semantic Constraint

**File:** `src/lib/regulatory-truth/prompts/index.ts:224`

```
"explanation_hr": "string (Croatian explanation)",
```

No instruction to:

- Match explanation to exactQuote semantics
- Avoid modal verbs not in source
- Only include dates/values present in source

### Path 4: Missing Quote-to-Answer Semantic Check

**File:** `src/lib/assistant/validation.ts`

The `validateCitationsForRegulatory()` function (lines 70-102) validates:

- `url` non-empty
- `quote` non-empty
- `evidenceId` non-empty
- `fetchedAt` non-empty

But does NOT validate:

- `directAnswer` content matches `quote` semantically
- No modal verb divergence
- No value/date additions

---

## Concrete Hallucination Scenarios

### Scenario A: Modal Verb Injection

**Source quote:** "Stopa PDV-a iznosi 25%"
**Generated explanation:** "Porezni obveznici **moraju** primjenjivati stopu PDV-a od 25%"
**Problem:** "moraju" (must) added without source support

### Scenario B: Temporal Expansion

**Source quote:** "prag za paušalno oporezivanje iznosi 39.816,84 EUR"
**Generated explanation:** "Prag vrijedi **od 1. siječnja 2025.** i primjenjuje se na **cjelokupni godišnji prihod**"
**Problem:** Date and scope added without evidence

### Scenario C: Scope Broadening

**Source quote:** "E-račun je obvezan za transakcije s javnim sektorom"
**Generated explanation:** "E-račun je obvezan za **sve** poslovne transakcije u Hrvatskoj"
**Problem:** "sve" (all) expands narrow scope to universal

---

## Recommendations (Out of Scope for This Audit)

The following fixes are recommended but not implemented as part of this audit:

1. **Semantic validator:** Compare explanationHr against exactQuote using semantic similarity
2. **Modal verb filter:** Reject explanationHr containing modal verbs not in source
3. **Value extraction check:** Validate all numbers/dates in explanationHr exist in extractedValue
4. **Confidence source count:** Factor source pointer count into confidence badge
5. **Quote-to-answer alignment test:** Add test asserting directAnswer ⊆ exactQuote semantically

---

## Test Coverage Analysis

**Existing tests validate:**

- Citation structure (primary.url, primary.quote, primary.evidenceId)
- Length limits
- Schema version
- Kind/topic combinations

**Missing tests:**

- Semantic alignment between directAnswer and quote
- Modal verb containment
- Value/date traceability

---

## Verdict

**FAIL**

The answer composition layer provides strong structural guarantees but lacks semantic guards. Natural-language answers (via `explanationHr`) can introduce modal verbs, expand scope, and add dates/obligations not present in cited evidence. The LLM-generated explanation text is stored and displayed without validation against the source quote.

**Risk level:** HIGH for regulatory compliance product

**Affected invariants:** 1, 2, 3 (FAIL), 4 (PARTIAL FAIL)

---

## Appendix: Files Reviewed

| File                                                  | Lines | Purpose                 |
| ----------------------------------------------------- | ----- | ----------------------- |
| `src/lib/assistant/query-engine/answer-builder.ts`    | 517   | Main answer composition |
| `src/lib/assistant/query-engine/citation-builder.ts`  | 48    | Citation assembly       |
| `src/lib/assistant/query-engine/rule-selector.ts`     | 81    | Rule retrieval          |
| `src/lib/assistant/query-engine/conflict-detector.ts` | 82    | Conflict detection      |
| `src/lib/assistant/query-engine/query-interpreter.ts` | 811   | Query interpretation    |
| `src/lib/assistant/validation.ts`                     | 149   | Response validation     |
| `src/lib/assistant/types.ts`                          | 276   | Type definitions        |
| `src/lib/regulatory-truth/agents/composer.ts`         | 493   | Rule composition        |
| `src/lib/regulatory-truth/agents/reviewer.ts`         | 338   | Rule review             |
| `src/lib/regulatory-truth/prompts/index.ts`           | 424   | LLM prompts             |
| `src/lib/regulatory-truth/schemas/composer.ts`        | 114   | Composer schema         |
| `src/lib/regulatory-truth/schemas/reviewer.ts`        | 84    | Reviewer schema         |
